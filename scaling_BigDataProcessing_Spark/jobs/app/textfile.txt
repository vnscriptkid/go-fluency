The difference between the two scripts is that the first script uses the Resilient Distributed Dataset (RDD) API, and the second script uses the DataFrame API.

In the first script, the `lines` variable is an RDD, which is a fundamental data structure of Spark. It is an immutable distributed collection of objects, which can be processed in parallel. We perform transformations on this RDD (like `map` and `flatMap`) and an action (`countByValue`) to get the word counts.

The second script, on the other hand, works with DataFrames, which are a more high-level abstraction and are a part of the Spark SQL module. They are distributed collection of data organized into named columns, much like a table in a relational database. The script performs transformations using DataFrame operations and SQL-like functions.

There are a few differences between the RDD and DataFrame APIs:

1. **Ease of use**: DataFrames provide a higher level of abstraction and are often easier to work with, especially for those who are familiar with SQL or data analysis in Python (with pandas).

2. **Performance**: DataFrames can have better performance than RDDs, because they have an optimized execution engine (Catalyst) that can generate efficient plans using techniques like predicate pushdown and column pruning. They can also take advantage of built-in functions that are typically more optimized than custom transformations on RDDs.

3. **Functionality**: DataFrames support a wider range of operations, including complex aggregations, window functions, and can directly use SQL queries.

The choice between RDDs and DataFrames depends on your specific use case and preference. In many cases, you can achieve the same result with either API. For complex data analysis tasks and when you need high-level abstractions and optimizations, DataFrames are generally a better choice. However, for some low-level transformations and actions, you might need the flexibility of RDDs.